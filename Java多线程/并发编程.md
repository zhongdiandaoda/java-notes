# 1 overview

## 1.1 上下文切换

CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。

在数据量较小时，并发的速度比串行要慢，原因就是线程有创建和上下文切换的开销。

减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。

- 无锁并发编程：将数据分段，不同的线程处理不同段的数据（Concurrent Hash Map）
- CAS算法：Java的Atomic包使用CAS算法来更新数据，而不需要加锁。
- 使用最少线程：线程池技术。
- 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。

## 1.2 悲观锁和乐观锁

锁的一种宏观分类方式是悲观锁和乐观锁。

悲观锁（Pessimistic Lock）：总是假设最坏的情况，每次取数据时都认为其他线程会修改该数据，所以每次在操作数据的时候都会上锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在进行相关操作之前先上锁。Java中synchronized和`ReentrantLock`等独占锁就是悲观锁思想的实现。

乐观锁（Optimistic Lock）：总是假设最好的情况，每次取数据时都认为其他线程不会修改该数据，所以不会上锁，但是在更新的时候会判断在此期间内，其他线程有没有更新过这个数据，可以使用版本号机制和CAS算法实现。在Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。

**悲观锁阻塞事务，乐观锁回滚重试。**

悲观锁适用于写多读少的情况，而乐观锁适用于读多写少的情况。

## 1.3 CAS算法

无锁算法Compare And Swap。

独占锁是一种悲观锁，synchronized就是一种独占锁，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。另一个更加有效的锁是乐观锁。即每次不加锁，而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁用到的机制就是CAS。

CAS是一种无锁算法，CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。`java.util.concurrent`包全完建立在CAS之上。

![img](并发编程.assets/v2-81e2d1e2b3b5a371118ec4a52a9ad264_b.jpg)

CPU去更新一个值时，先将备份的值与内存中的值进行比较，如果值不同，说明在备份值到写入新值之前，有其他线程对该值进行了修改，则取消本次操作。

CAS的缺点：

- ABA问题：因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。从Java1.5开始JDK的atomic包里提供了一个类`AtomicStampedReference`来解决ABA问题。这个类的`compareAndSet`方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。
- 循环时间长，开销大：当CAS长时间不成功时，会给CPU带来非常大的执行开销。如果CAS写入不成功将触发等待 -> 重试机制，这种情况是一个自旋锁，简单来说就是适用于短期内获取不到，进行等待重试的锁，它不适用于长期获取不到锁的情况，另外，自旋循环对于性能开销比较大。
- 只能保证一个共享变量的原子操作：CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。从 JDK 1.5开始，提供了`AtomicReference`类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行 CAS 操作。所以我们可以使用锁或者利用`AtomicReference`类把多个共享变量合并成一个共享变量来操作。

## 1.3 互斥锁

mutex（mutual   exclusive）即互斥量（互斥体）。

```
//要保证函数体为原子操作

mutex_lock(mutex) {
    lock(bus);    //给总线加锁

    mutex = mutex - 1;
    if(mutex != 0)
        block()
    else
        success

    unlock(bus);  
}


mutex_unlock(mutex) {
    lock(bus);

    mutex = mutex + 1;
    if (mutex != 1) 
        wakeup();
    else
        success
    
   	unlock(bus);
}
```

## 1.4 自旋锁spinlock

```
void spin_lock(int * owner ) {
    while(!compare_and_set(owner, 0, 1) {
    }
}
```

## 1.5 `epoll`

`epoll`是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。

`epoll`采用了红黑树来对它管理的FD集合作索引。

功能：加入新连接；检测部分节点有数据可读。

使用哈希的问题：需要多次扩容

使用B+数的问题：B+树主要用于磁盘索引，目的是减少寻址次数。

# 2 Java并发机制的底层实现

Java有两种加锁的方式，一种是synchronized关键字，另一种是lock接口的实现类。

![img](并发编程.assets/v2-ddb71ab0b68d65ae70244bfdeb0d6704_b.jpg)

**`ReentrantLock`、`ReadLock`、`WriteLock`** 是Lock接口最重要的三个实现类。对应了“可重入锁”、“读锁”和“写锁”。

## 2.1 volatile

在多线程并发编程中，volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。即一个线程在修改一个共享变量时，另一个线程可以读到这个修改的值。

volatile的使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。

Java语言提供了volatile，在某种情况下比加锁更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的，Java语言保证使用volatile关键字修饰的变量在读取时，必须从主内存中获取最新的值，写入时一定实时写入到主内存（工作内存和主内存是JMM中的抽象概念，不一定对应内存和cache）。

使用volatile

```java
class demo {
	volatile int i = 1;
    public void set(int num) {
        i = num;
    }
    public void inc() {
        i++;
    }
    public void get() {
        return i;
    }
}
```

使用锁

```java
class demo {
	int i = 1;
    public synchronized void set(int num) {
        i = num;
    }
    public void inc() {
        int tmp = get();
        tmp = tmp + 1;
        set(tmp);
    }
    public synchronized void get() {
        return i;
    }
}
```

上面的两段代码是等价的，对于get和set操作，使用volatile变量和使用锁的效果是一致的，但是对于`inc`操作，多线程环境下这样的操作不具有原子性。

### volatile底层原理

未优化的缓存一致性协议能够保证并发编程的可见性，但加入了写缓冲器和无效队列之后，必须由编译器在生成的机器码中插入内存屏障才能保证可见性。

> 内存屏障（memory barriers）：一组处理器指令，用于实现对内存操作的顺序限制。
>
> 缓存行（cache line）：CPU高速缓存中可以分配的最小存储单位。处理器填写缓存行时会加载整个缓存行。

#### **缓存一致性协议**

为了提高处理器的执行速度，在处理器和内存之间增加了多级缓存来提升。但是由于引入了多级缓存，就存在缓存数据不一致问题。

对于单核CPU，通常有两种方式：

- **通写法(Write Through)**：每次cache中的内容被修改后立即写入到内存。
- **写回法(Write Back)**：cache 中内容被修改后，延迟写入内存。当cache和内存数据不一致时以cache中的数据为准。

对于多核CPU来说，cache与主存的内容同步可能会存在多线程竞争问题，又引入了以下操作：

- **写失效**：当一个CPU修改了数据，其他CPU中的该数据失效
- **写更新**：当一个CPU修改了数据，通知其他CPU对该数据进行更新

在CPU层面，提供了两种解决方案：

- **总线锁**：在多CPU情况下，某个CPU对共享变量进行操作时对总线加锁，其他CPU不能对该变量进行读写，会影响CPU性能。
- **缓存锁**：降低了锁的粒度，基于缓存一致性协议。

缓存一致性协议：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。

**缓存一致性协议需要满足两种特性：**

1. 写序列化：缓存一致性协议要求总线上任意时间只能出现一个CPU写事件，多核并发的写事件会通过总线仲裁机制转换成串行化的写事件序列。
2. 写传播：一个处理器的写操作对其他处理器可见。
   - 方式一：嗅探，CPU监听总线上的所有活动。
   - 方式二：基于目录，总线事件仅发送给需要接受的CPU。

通常使用的缓存一致性协议为MESI(Modified Exclusive Shared Invalid)，实现了写回法，写失效，缓存行锁，写传播，写序列化和嗅探机制。

四种状态：

- M: 被修改（Modified)

  当前 CPU 缓存有最新数据， 其他 CPU 拥有失效数据，当前 CPU 数据与内存不一致，但以当前 CPU 数据为准。

- E: 独享的（Exclusive)

  只有当前 CPU 有数据，其他 CPU 没有该数据，当前 CPU 数据与内存数据一致。

- S: 共享的（Shared)

  当前 CPU 与其他 CPU 拥有相同数据，并与内存中数据一致。

- I: 无效的（Invalid）

  当前 CPU 数据失效，其他 CPU 数据可能有可能无，数据应从内存中读取，且当前 CPU 与 内存数据不一致。

处理器对缓存的请求：

- `PrRd`：CPU 读操作
- `PrWr`：CPU 写操作

总线对缓存的请求：

- `BusRd`: 窥探器请求指出其他处理器请求**读**一个缓存块
- `BusRdX`: 窥探器请求指出其他处理器请求**写**一个该处理器不拥有的缓存块
- `BusUpgr`: 窥探器请求指出其他处理器请求**写**一个该处理器拥有的缓存块
- `Flush`: 窥探器请求指出请求**回写**整个缓存到主存
- `FlushOpt`: 窥探器请求指出整个缓存块被发到总线以发送给另外一个处理器（缓存到缓存的复制）

不同状态时，执行不同操作，会产生不同的状态转移。

**当前CPU状态为Modified**

- `PrRd`：直接从缓存中读取数据，无总线事务生成，状态不变。
- `PrWr`：直接修改当前 CPU 缓存数据，无总线事务生成，状态不变。

**当前状态为Exclusive**

- `PrRd`：无总线事务生成，状态不变。
- `PrWr`：修改当前 CPU 缓存值，无总线事务生成，状态改为 M。

**当前状态为Shared**

- `PrRd`：状态不变，无总线事务生成。
- `PrWr`：发出总线事务`BusUpgr`信号，状态改为M，其他缓存看到`BusUpgr`信号时标记缓存行为Invalid。

**当前CPU状态为Invalid**

- `PrRd`： CPU 缓存不可用，需要读内存。给总线发出`BusRd`信号，其他处理器看到`BusRd`，检查自己是否有失效的数据副本，向发送者回复Response。
  - 如果其他缓存有有效的副本，则状态转换为Shared。
  - 如果其他缓存都没有有效的副本，则从主存读取数据，状态转换为Exclusive。
- `PrWr`：当前 CPU 缓存不可用，需要写内存。给总线发出`BusRdX`信号，状态转换为Modified。
  - 如果其他缓存有有效的副本，则从其中一个缓存中获取数据，并向缓存块中写入修改后的值。
  - 否则，从主存中获取数据。

总线操作的状态转化：

| 初始状态  | 操作             | 响应                                                      |
| --------- | ---------------- | --------------------------------------------------------- |
| Invalid   | `BusRd`          | 状态保持不变，信号忽略                                    |
| Invalid   | `BusRdX/BusUpgr` | 状态保持不变，信号忽略                                    |
| Exclusive | `BusRd`          | 状态变为共享<br>发出总线`FlushOpt`信号并发出块的内容      |
| Exclusive | `BusRdX`         | 状态变为无效<br/>发出总线`FlushOpt`信号并发出块的内容     |
| Shared    | `BusRd`          | 状态变为共享<br/>可能发出总线`FlushOpt`信号并发出块的内容 |
| Shared    | `BusRdX`         | 状态变为无效<br/>可能发出总线`FlushOpt`信号并发出块的内容 |
| Modified  | `BusRd`          | 状态变为共享<br/>发出总线`FlushOpt`信号并发出块的内容     |
| Modified  | `BusRdX`         | 状态变为无效<br/>发出总线`FlushOpt`信号并发出块的内容     |

写操作仅在缓存行是已修改或独占状态时可自由执行。如果在共享状态，其他缓存都要先把该缓存行置为无效，这种广播操作称作*Request For Ownership (RFO)*。



**MESI的问题：**

在 MESI 中，依赖总线嗅探机制，整个过程是串行的，可能会发生阻塞。

1. 若 CPU0 发生写操作，首先需要发送一个 Invalidate 消息给到其他缓存了该数据的 CPU。并且要等待其他 CPU 的确认回执。CPU0 在这段时间内都会处于阻塞状态。
2. 对于 CPU 收到总线的读信号，需要失效缓存。当其高速缓存压力很大时，要求实时的处理失效事件也存在一定的困难，会有一定的延迟。

**为了解决MESI中的处理器等待问题，引入了写缓冲区和失效队列。**

#### 写缓冲区与失效队列

**写缓冲区Store Buffer**

写缓冲区是每个 CPU 私有的一块比高速缓存还小的存储部件，当使用了写缓冲区后，每当发生 LW，当前 CPU 不再阻塞地等待其他 CPU 的确认回执，而是直接将更新的值写入写缓冲区，然后继续执行后续指令，随后在某个时刻异步将数据写入到主存。

**存储转发Store Forwarding**

在进行 LR 时，CPU 会先在写缓冲区中查询记录是否存在，如果存在则会从写缓冲区中直接获取。

写缓冲区帮助处理器实现了异步写数据的能力，使得处理器处理指令的能力大大提升。

**失效队列**

失效队列也是每个 CPU 私有的，使用失效队列后，发生总线读事务时对应的 CPU 缓存不再同步地失效缓存并发送确认回执，而是将失效消息放入失效队列，立即发送ACK。

后续 CPU 会在空闲时对失效队列中的消息进行处理，将对应的 CPU 缓存失效。

失效队列帮助解决了删除数据等待的问题。

**写缓冲区和失效队列虽然解决了缓冲一致性协议执行时，由于总线事务导致的CPU等待问题，但又导致了内存系统重排序（伪重排序）和可见性问题。**

由于写缓冲器和无效化队列的出现，处理器的执行都变成了异步操作。缓冲器是每个处理器私有的，一个处理器所存储的内容是无法被其他处理器读取的。

例如：

可见性问题

CPU1 更新变量到写缓冲器中，而 CPU2 在收到 Invalidate 消息后，回复ACK，并向无效化队列写入一条无效化缓存的消息，当 CPU2 还未消费无效化队列的信息，并读取变量时，读到的依然是旧值；或者 CPU1 在未收到全部 ACK，修改的数据仍位于写缓冲中时，CPU3 进行读操作，读到的仍然是旧值。

伪重排序问题

Store-Load重排序，对于代码：

```
int a = 10;
int b = 2;
void f() {
	a = 20;
	int c = b;
}
```

当 CPU 将对变量 a 的写入记录到写缓冲中，且写缓冲中的消息还未被消费，此时再执行第二条语句，则会导致代码的执行顺序看起来变为了 2 -> 1，而这是由于写缓冲区没有及时写入 cache 导致的，有的观点也将这种由于可见性导致的重排序称为内存系统重排序。

处理器在写缓冲器满、I/O指令被执行时会将写缓冲器中的内容写入高速缓存中。但从变量更新角度来看，处理器本身无法保障这种更新的”及时“性。为了保证处理器对共享变量的更新可被其他处理器同步，编译器等底层系统借助一类称为内存屏障的特殊指令来实现。

#### 内存屏障

**内存屏障**（英语：Memory barrier），也称**内存栅栏**，**内存栅障**，**屏障指令**等，是一类同步屏障指令，它使得 CPU 或编译器在对内存进行操作的时候, 严格按照一定的顺序来执行, 也就是说在memory barrier 之前的指令和memory barrier之后的指令不会由于系统优化等原因而导致乱序。

内存屏障的类型

| 屏障类型              | 指令示例                   | 说明                                                         |
| :-------------------- | :------------------------- | :----------------------------------------------------------- |
| `LoadLoad Barriers`   | `Load1;LoadLoad;Load2`     | 该屏障确保Load1数据的装载先于Load2及其后所有装载指令的的操作 |
| `StoreStore Barriers` | `Store1;StoreStore;Store2` | 该屏障确保Store1立刻刷新数据到内存(使其对其他处理器可见)的操作先于Store2及其后所有存储指令的操作 |
| `LoadStore Barriers`  | `Load1;LoadStore;Store2`   | 确保Load1的数据装载先于Store2及其后所有的存储指令刷新数据到内存的操作 |
| `StoreLoad Barriers`  | `Store1;StoreLoad;Load2`   | 该屏障确保Store1立刻刷新数据到内存的操作先于Load2及其后所有装载装载指令的操作。它会使该屏障之前的所有内存访问指令(存储指令和访问指令)完成之后,才执行该屏障之后的内存访问指令 |

`StoreLoad Barriers`同时具备其他三个屏障的效果，因此也称之为`全能屏障`（`mfence,memory fence`），是目前大多数处理器所支持的；但是相对其他屏障，该屏障的开销相对昂贵。

不同的CPU架构对内存屏障的实现也不同。

> 在编译器层面，仅将volatile作为标记使用，取消编译层面的缓存和重排序。

如果硬件架构本身已经保证了内存可见性（如单核处理器、一致性足够的内存模型等），或者硬件架构本身不进行处理器重排序、有更强的重排序语义（能够分析多核间的数据依赖）、或在单核处理器上重排序，那么volatile就是一个空标记，不会插入相关语义的内存屏障。

如果不保证，仍以x86架构为例，JVM对volatile变量的处理如下：

- 在写volatile变量v之后，插入一个`sfence`。这样，`sfence`之前的所有store（包括写v）不会被重排序到`sfence`之后，`sfence`之后的所有store不会被重排序到`sfence`之前，禁用跨`sfence`的store重排序；且`sfence`之前修改的值都会被写回缓存，并标记其他CPU中的缓存失效。
- 在读volatile变量v之前，插入一个`lfence`。这样，`lfence`之后的load（包括读v）不会被重排序到`lfence`之前，`lfence`之前的load不会被重排序到`lfence`之后，禁用跨`lfence`的load重排序；且`lfence`之后，会首先刷新无效缓存，从而得到最新的修改值，与`sfence`配合保证内存可见性。

> 在另外一些平台上，JVM使用`mfence`代替`sfence`与`lfence`，实现更强的语义。

二者结合，共同实现了Happens-Before关系中的volatile变量规则。



### volatile内存语义的增强

为了让volatile的读写具有和锁的获取与释放相同的效果，JSR对volatile的语义进行了增强，在编译器层面，对volatile变量的读写与普通变量的读写重排序规则做出了一定的限制，与volatile在内存屏障上的优化结合，实现了对对某个未被锁保护的变量的访问操作进行排序。

| 是否能重排序 | 第二个操作 |             |             |
| ------------ | ---------- | ----------- | ----------- |
| 第一个操作   | 普通读/写  | volatile 读 | volatile 写 |
| 普通读/写    |            |             | NO          |
| volatile 读  | NO         | NO          | NO          |
| volatile 写  |            | NO          | NO          |

例如，对于双重检查锁定与延迟初始化模型：

```java
public class Singleton1_3 {
  private static volatile Singleton1_3 singleton = null;
  public int f1 = 1;   // 触发部分初始化问题
  public int f2 = 2;
  private Singleton1_3() {
  }
  public static Singleton1_3 getInstance() {
    if (singleton == null) {
      synchronized (Singleton1_3.class) {
        // must be a complete instance
        if (singleton == null) {
          singleton = new Singleton1_3();
        }
      }
    }
    return singleton;
  }
}
```

通过增强的volatile语义，new关键字，禁止对象初始化与实例引用赋值两个操作之间的重排序，能够防止其他线程在某个线程已经进入临界区时，获取到值为null的引用变量。

### volatile不保证原子性

volatile可以保证单个变量的读/写具有原子性，但不保证一个代码块的原子性。

volatile方式的`i++`，总共是四个步骤：load、Increment、store、Memory Barriers。

内存屏障是线程安全的,但是内存屏障之前的指令并不是。在某一时刻线程1将 i 的值load取出来，放置到CPU缓存中，然后再将此值放置到寄存器A中，然后A中的值自增1（寄存器A中保存的是中间值，没有直接修改i，因此其他线程并不会获取到这个自增1的值）。如果在此时线程2也执行同样的操作，获取值i=10,自增1变为11，然后马上刷入主内存。此时由于线程2修改了i的值，实时的线程1中的i=10的值缓存失效，重新从主内存中读取，变为11。接下来线程1恢复。将自增过后的A寄存器值11赋值给CPU缓存i。这样就出现了线程安全问题。

### **总结**

1. 为了解决CPU与主存速度的不匹配，引入了cache，但导致了缓存一致性问题。
2. 为了解决缓存一致性问题，引入了MESI(缓存一致性协议)，但总线事务等待其他CPU的响应导致了CPU等待的问题
3. 引入了写缓冲队列和无效化队列，但导致了重排序和一致性问题
4. 引入内存屏障解决重排序与一致性问题。

可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。对于X86处理器，当一个线程修改一个volatile类型的共享变量时，JVM会向处理器添加一条Lock前缀的指令，将cache中的该变量立即写入到内存中。

所以，如果一个变量被volatile所修饰的话，在每次数据变化之后，其值都会被强制刷入主存。而其他处理器的缓存由于遵守了缓存一致性协议，也会把这个变量的值从主存加载到自己的缓存中。这就保证了一个volatile在并发编程中，其值在多个缓存中的一致性。

在逻辑层面上，Java内存模型规定了所有的变量都存储在主内存中，每条线程有自己的工作内存，线程的工作内存中保存了该线程中是用到的变量的主内存副本，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量的传递均需要自己的工作内存和主存之间进行数据同步进行。所以，就可能出现线程1改了某个变量的值，但是线程2不可见的情况，Java中的volatile关键字提供了一个功能，被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次使用前都从主内存刷新。

**工作内存和主内存都是抽象的概念，具体实现由JVM决定。**

JVM在处理volatile关键字时，首先对Java编译器重排序做出一定限制，再使用内存屏障对CPU重排序做出一定限制，最后通过MESI协议的支持，实现volatile的效果。









## 2.2. synchronized的实现原理和应用

未优化的synchronized被称为重量级锁，需要在用户态和内核态之间切换，效率很低。

synchronized实现同步的基础：

- 对于普通同步方法，锁是当前实例对象。

  ```java
  synchronized void method() {
      //业务代码
  }
  ```

- 对于静态同步方法，锁是当前类的Class对象。

  ```java
  synchronized static void method() {
      //业务代码
  }
  ```

- 对于同步方法快，锁是Synchronized括号里配置的对象。

  ```java
  synchronized(this) {
      //业务代码
  }
  ```

当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁，JVM基于进入和退出Monitor对象来实现同步和代码块同步。

JVM在编译后将`monitorenter`指令插入到同步代码块的开始位置，`monitorexit`插入到方法结束处和异常处，JVM要保证每个`monitorenter`与对应的`monitorexit`配对。

> 在 Java 虚拟机(`HotSpot`)中，Monitor 是基于 C++实现的，由[ObjectMonitor](https://github.com/openjdk-mirror/jdk7u-hotspot/blob/50bdefc3afe944ca74c3093e7448d6b889cd20d1/src/share/vm/runtime/objectMonitor.cpp)实现的。每个对象中都内置了一个 `ObjectMonitor`对象。
>
> 另外，`wait/notify`等方法也依赖于`monitor`对象，这就是为什么只有在同步的块或者方法中才能调用`wait/notify`等方法，否则会抛出`java.lang.IllegalMonitorStateException`的异常的原因。

在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。

在执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。

`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。

demo（双重校验锁实现对象那个单例）：

```java
public class Singleton {
	//volatile 防止指令重排
    private volatile static Singleton uniqueInstance;
    private Singleton() {
    }
    public  static Singleton getUniqueInstance() {
       //先判断对象是否已经实例过，没有实例化过才进入加锁代码
        if (uniqueInstance == null) {
            //类对象加锁
            synchronized (Singleton.class) {
                //两个if，防止多个线程轮流执行synchronized方法
                if (uniqueInstance == null) {
                    uniqueInstance = new Singleton();
                }
            }
        }
        return uniqueInstance;
    }
}
```

**构造方法本身就是线程安全的，不能使用synchronized修饰。**

### 1 Java对象头

synchronized用的锁是存在对象头里的。如果对象是数组类型，则JVM用3个Word存储对象头，如果对象是非数组类型，则JVM用两个Word存储对象头。一个Word等于4字节。

| 长度     | 内容                   | 说明                             |
| -------- | ---------------------- | -------------------------------- |
| 32/64bit | Mark Word              | 对象的`hashcode`或锁信息等       |
| 32/64bit | Class Metadata Address | 指向方法区中对象的类型数据       |
| 32/64bit | Array Length           | 数组的长度（如果对象是数组类型） |

Mark Word中会默认存储对象的`HashCode`、GC分代年龄和锁标记位。

| 锁状态   | 25bit      | 4bit       | 1bit是否是偏向锁 | 2bit锁标志位 |
| -------- | ---------- | ---------- | ---------------- | ------------ |
| 无锁状态 | `HashCode` | GC分代年龄 | 0                | 01           |

Mark Word中存储的数据会随着锁标志位的变化而变化。

**无锁状态**

| 25bit          | 4bit         | 1bit(是否是偏向锁) | 2bit(锁标志位) |
| -------------- | ------------ | ------------------ | -------------- |
| 对象的hashCode | 对象分代年龄 | 0                  | 01             |

**偏向锁状态**

| 23bit  | 2bit  | 4bit         | 1bit | 2bit |
| ------ | ----- | ------------ | ---- | ---- |
| 线程ID | epoch | 对象分代年龄 | 1    | 01   |

**轻量级锁状态**

| 30bit                | 2bit |
| -------------------- | ---- |
| 指向栈中锁记录的指针 | 00   |

**重量级锁状态**

| 30bit                      | 2bit |
| -------------------------- | ---- |
| 指向互斥量(重量级锁)的指针 | 10   |

### 2 锁的升级与对比

JDK1.6对锁的实现引入了大量的优化，JDK1.6中，锁一共有4种状态，从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。

**1 偏向锁**

`HotSpot`的作者发现在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。

**加锁**

JVM中的每个类有一个类似mark word的prototype_header，用来标记该class的epoch和偏向开关等信息。

加锁时会在栈中申请一个Lock Record。

![img](%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/177141-20191103161341771-175207166.png)

工作流程：

设需要加锁的对象是o，其类型为T。

1. 从当前线程的栈中找到一个空闲的`Lock Record`，判断`Lock Record`是否空闲的依据是其`obj`字段是否为null。注意这里是按内存地址从低往高找到最后一个可用的`Lock Record`，换而言之，就是找到内存地址最高的可用`Lock Record`。
2. 获取到`Lock Record`后，为其`obj`字段赋值o。
3. 判断锁对象的Mark Word是否是偏向锁模式，即低3bit是否为101。
4. `if`偏向的线程是当前线程且Mark Word的epoch字段等于class的epoch字段，则直接执行同步代码。
5. `else if` class的prototype_header中偏向模式是关闭的，则说明class执行了批量撤销，进入轻量级锁的逻辑。
6. `else if` 对象中的epoch不等于class中的epoch，说明程序执行了批量重偏向，利用CAS将锁对象的mark word替换为一个偏向当前线程且epoch为类的epoch的新的mark word。如果CAS失败，说明存在竞争，则撤销偏向锁。
7. `else `，执行到此处时，说明当前要么偏向别的线程，要么是匿名偏向（即没有偏向任何线程），构建一个Mark Word，尝试CAS替换掉锁对象的mark word，如果修改成功，则执行同步代码块，否则，撤销偏向锁。

**撤销锁**

偏向锁使用了一种等到竞争出现才释放锁的机制。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。

它会首先暂停拥有偏向锁的线程A，然后判断这个线程A，此时有两种情况：
1：A 线程已经退出了同步代码块，或者是已经不在存活了，如果是上面两种情况之一的，此时就会直接
撤销偏向锁，变成无锁状态。
2：A 线程还在同步代码块中，此时将 A 线程的偏向锁升级为轻量级锁。

**批量重偏向与撤销**

从偏向锁的加锁解锁过程中可以看出，当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到`safe point`时将偏向锁撤销为无锁状态或升级为轻量级/重量级锁。当运行时的场景本身存在多线程竞争的，那偏向锁的存在不仅不能提高性能，而且会导致性能下降。因此，JVM中增加了一种批量重偏向/撤销的机制。

存在如下两种情况：

1.一个线程创建了大量对象并执行了初始的同步操作，之后在另一个线程中将这些对象作为锁进行之后的操作。这种case下，会导致大量的偏向锁撤销操作。

2.存在明显多线程竞争的场景下使用偏向锁是不合适的，例如生产者/消费者队列。

批量重偏向（`bulk rebia`s）机制是为了解决第一种场景。批量撤销（bulk revoke）则是为了解决第二种场景。

其做法是：以class为单位，为每个class维护一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象会有一个对应的`epoch`字段，每个处于偏向锁状态对象的`mark word`也有该字段，其初始值为创建该对象时，class中的`epoch`的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其`epoch`字段改为新值。下次获得锁时，发现当前对象的`epoch`值和class的`epoch`不相等，则直接通过CAS操作将其`mark word`的Thread Id 改成当前线程Id。

当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑。



**2 轻量级锁**

当出现竞争现象时，偏向锁会升级为轻量级锁。

**加锁**

线程在执行同步块之前，JVM会先在当前线程的栈帧中创建用户存储锁记录的空间，并将对象头中的MarkWord复制到锁记录中。然后线程尝试使用CAS将对象头中的MarkWord替换为指向锁记录的指针。如果成功, 当前线程获得锁；否则，表示其它线程竞争锁，当前线程便尝试使用自旋来获取锁。

**轻量级锁解锁**

轻量级锁解锁时，会使用原子的CAS操作将当前线程的锁记录替换回到对象头，如果成功，表示没有竞争发生；如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。

**总结**

总结一下加锁解锁过程，有线程A和线程B来竞争对象c的锁。这时线程A和线程B同时将对象c的MarkWord复制到自己的锁记录中，两者竞争去获取锁。假设线程A成功获取锁，并将对象c的对象头中的线程ID(MarkWord中)修改为指向自己的锁记录的指针，这时线程B仍旧通过CAS去获取对象c的锁，因为对象c的MarkWord中的内容已经被线程A改了，所以获取失败。此时为了提高获取锁的效率，线程B会循环去获取锁，这个循环是有次数限制的，如果在循环结束之前CAS操作成功，那么线程B就获取到锁，如果循环结束依然获取不到锁，则获取锁失败，对象c的MarkWord中的记录会被修改为重量级锁，然后线程B就会被挂起，之后有线程C来获取锁时，看到对象c的MarkWord中的是重量级锁的指针，说明竞争激烈，直接挂起。

解锁时，线程A尝试使用CAS将对象c的MarkWord改回自己栈中复制的那个MarkWord，因为对象c中的MarkWord已经被指向为重量级锁了，所以CAS失败. 线程A会释放锁并唤起等待的线程，进行新一轮的竞争。

![image-20210719211558871](并发编程.assets/image-20210719211558871.png)



## 2.3 原子操作的实现原理

CPU需要保证检测锁的状态并获取锁的动作的原子性，即在某一时刻只有一个进程可以获取到锁。一般是通过对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。

### 2.3.1 总线锁

多个处理器同时对共享变量进行读写操作，如`i++`时，操作后共享变量的值可能会与期望的不一致。

原因是多个处理器同时从内存中读取一个变量`i`并保存到缓存，然后+1，最后写入到系统内存中。处理器是使用总线锁来解决这个问题的，当一个处理器在总线上输出LOCK#信号时，其他处理器的请求将被阻塞。

### 2.3.2 缓存锁

总线锁定期间，其他CPU无法与内存进行通信，性能浪费比较严重，目前处理器在某些场合下使用缓存锁来代替总线锁进行优化。

Java中volatile关键字的原理就是缓存锁。

不能使用缓存锁的情况：

- 操作的数据不能被缓存在CPU内部，或操作的数据跨多个缓存行；
- 处理器不支持缓存锁定。

### 2.3.3 Java如何实现原子操作

Java中通过循环CAS来实现原子操作。自旋CAS实现的基本思路是循环进行CAS操作直到成功为止。

demo：基于CAS实现线程安全的计数器方法`safeCount`和一个非线程安全的计数器count。

```java
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

public class Solution {
    private AtomicInteger atomicInteger = new AtomicInteger(0);
    private int i = 0;
    public static void main(String[] args) {
        final Solution solution = new Solution();
        List<Thread> ts = new ArrayList<>(600);
        for (int j = 0; j < 100; j++) {
            Thread t = new Thread(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10000; i++) {
                        solution.count();
                        solution.safeCount();
                    }
                }
            });
            ts.add(t);
        }
        for(Thread t : ts) {
            t.start();
        }
        for(Thread t : ts) {
            try {
                t.join();
            }catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
        System.out.println(solution.i);
        System.out.println(solution.atomicInteger.get());
    }
    private void safeCount() {
        for(;;) {
            int i = atomicInteger.get();
            boolean suc = atomicInteger.compareAndSet(i, ++i);
            if(suc) {
                break;
            }
        }
    }
    private void count() {
        i++;
    }
}
//output:
//998380
//1000000
```

**除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。**

# Java内存模型

## Overview

并发编程需要处理的两个关键问题是：线程之间如何同步；线程之间如何通信。

Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。JMM是一个逻辑上的概念，在物理内存中并不一定实际存在。

<img src="并发编程.assets/image-20210720225208310.png" alt="image-20210720225208310" style="zoom:50%;" />

Java线程之间的通信由Java内存模型（JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。JMM定义了线程和内存之间的抽象关系：线程之间的共享变量存储在主内存，每个线程有一个私有的本地内存，存储该线程读/写共享内存的副本。JMM规定线程对变量的所有的操作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量，不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。

JMM的这种规定导致了可见性问题。

线程的工作内存中保存着线程的方法调用信息和方法的局部变量（仅当前线程可见），主内存中保存着对象类型。

![The Java Memory Model showing where local variables and objects are stored in memory.](并发编程.assets/java-memory-model-2.png)

硬件内存架构：

![Modern hardware memory architecture.](并发编程.assets/java-memory-model-4.png)

JMM与硬件内存架构的映射：

![The division of thread stack and heap among CPU internal registers, CPU cache and main memory.](并发编程.assets/java-memory-model-5.png)

由于对象和变量会存储在内存的不同区域，导致了以下的问题：

- 共享变量的可见性
- 读写共享变量的竞争

Java使用了volatile和锁来解决这两个问题。



## 指令的重排序

在执行程序时，为了提高性能，编译器和处理器常常会对指令作重排序，重排序分三种类型：

- 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，重新安排语句的执行顺序。
- 指令级并行的重排序
- 内存系统的重排序

从Java源代码到最终执行的指令序列，会分别经历这三种重排序：

![image-20210720225649160](并发编程.assets/image-20210720225649160.png)

2和3属于处理器重排序，可能会导致多线程程序出现内存可见性问题。JMM会禁止特定类型的编译器重排序，并插入特定类型的内存屏障指令，来禁止指定类型的处理器重排序。

数据依赖性：

- 写后读
- 读后写
- 写后写

编译器和处理器对操作进行重排序时，会遵守数据依赖性，不会改变存在数据依赖关系的两个操作的执行顺序，但重排序不会考虑多个CPU或线程之间的数据依赖性。

**AS-IF-SERIAL语义：**指的是重排序保证单线程情况下程序执行结果的正确性。

重排序对多线程的影响：

```java
class ReorderExample {
	int a = 0;
	boolean flag = false;
	public void writer() {
		a = 1;
		flag = true;
	}
	public void reader() {
		if(flag) {
			int i = a * a;
			.....
		}
	}
}
```

线程A执行writer，线程B执行reader时，可能会引发语义错误：

<img src="并发编程.assets/image-20210826170739298.png" alt="image-20210826170739298" style="zoom:50%;" />



顺序一致性模型指的是一个线程中的所有操作按照程序顺序来执行，在多线程情况下，整体上无序，但同一个线程的指令是按序执行的，如`A1->B1->B2->B3->A2->A3`。

JMM不保证顺序一致性，未同步的程序在JMM中整体无序，且单个线程的指令也可能是无序的。



按照是否真正发生了指令的重排序，也可将重排序分为：

- 真重排序：由编译器、处理器出于优化的目的，对指令所做的重排序。
- 伪重排序：由于可见性问题，导致指令没有重排序，但出现了重排序的效果。





## happens-before规则



